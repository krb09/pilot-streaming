{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Pilot-Streaming on Stampede\n",
    "\n",
    "In the first step we need to import all required packages and modules into the Python Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pilot-Streaming utilizes [SAGA-Python](http://saga-python.readthedocs.io/en/latest/tutorial/part3.html) to manage the Spark cluster environment. All attributes of the SAGA Job map 1-to-1 to the Pilot Compute Description. \n",
    "\n",
    "`resource`: URL of the Local Resource Manager. All SAGA adaptors are supported. Examples:\n",
    "\n",
    "* `slurm://localhost`: Submit to local SLURM resource manager, e.g. on master node of Wrangler or Stampede\n",
    "* `slurm+ssh://login1.wrangler.tacc.utexas.edu`: Submit to Wrangler master node SLURM via SSH (e.g. on node running a job)\n",
    "\n",
    "`type:` The `type` attributes specifies the cluster environment. It can be: `Spark`, `Dask` or `Kafka`.\n",
    "\n",
    "\n",
    "Note: This is not required anymore on Stampede 2\n",
    "\n",
    "Depending on the resource there might be other configurations necessary, e.g. to ensure that the correct subnet is used the Spark driver can be configured using various environment variables:   os.environ[\"SPARK_LOCAL_IP\"]='129.114.58.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T17:59:21.386383Z",
     "start_time": "2017-12-28T17:59:21.364643Z"
    }
   },
   "outputs": [],
   "source": [
    "# System Libraries\n",
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "\n",
    "## logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pilot.streaming' from '/home1/06548/tg858476/anaconda3/lib/python3.7/site-packages/Pilot_Streaming-0.31.2-py3.7.egg/pilot/streaming.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# System Libraries\n",
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "## logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "import datetime\n",
    "import confluent_kafka\n",
    "from confluent_kafka import TopicPartition\n",
    "import pykafka\n",
    "import pyspark\n",
    "import time\n",
    "import redis\n",
    "import uuid\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "# Dask\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "from dask.delayed import delayed\n",
    "import distributed\n",
    "from distributed import Client\n",
    "# Pilot-Streaming\n",
    "import pilot.streaming\n",
    "sys.modules['pilot.streaming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_URL=\"slurm+ssh://login4.stampede2.tacc.utexas.edu\"\n",
    "WORKING_DIRECTORY=os.path.join(os.environ[\"HOME\"], \"work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_compute_description = {\n",
    "    \"resource\":RESOURCE_URL,\n",
    "    \"working_directory\": WORKING_DIRECTORY,\n",
    "    \"number_of_nodes\": 1,\n",
    "    \"cores_per_node\": 48,\n",
    "    \"project\": \"TG-MCB090174\",\n",
    "    \"queue\": \"normal\",\n",
    "    \"config_name\": \"stampede\",\n",
    "    \"walltime\": 59,\n",
    "    \"type\":\"kafka\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pilot-streaming:Pilot-Streaming SLURM: Parsing job description: {'executable': 'python', 'arguments': ['-m ', 'pilot.plugins.kafka.bootstrap_kafka', ' -n ', 'stampede'], 'working_directory': '/home1/06548/tg858476/work/kafka-dbd88714-e6d8-11e9-8599-15f796122f42', 'output': 'kafka_job_kafka-dbd88714-e6d8-11e9-8599-15f796122f42.stdout', 'error': 'kafka_job_kafka-dbd88714-e6d8-11e9-8599-15f796122f42.stderr', 'number_of_nodes': 1, 'cores_per_node': 48, 'project': 'TG-MCB090174', 'reservation': None, 'queue': 'normal', 'walltime': 59}\n",
      "DEBUG:pilot-streaming:Submit pilot job to: slurm+ssh://login4.stampede2.tacc.utexas.edu\n",
      "DEBUG:pilot-streaming:Type Job IDps-dbdb0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory: /home1/06548/tg858476/work\n",
      "/tmp/tmpurwqgtud\n",
      "Submission of Job Command: ssh login4.stampede2.tacc.utexas.edu sbatch  tmpurwqgtud\n",
      "Cleanup: ssh login4.stampede2.tacc.utexas.edu rm tmpurwqgtud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pilot-streaming:Pilot-Streaming SLURM: SSH run job finished\n",
      "DEBUG:pilot-streaming:Output - \n",
      "To access the system:\n",
      "\n",
      "1) If not using ssh-keys, please enter your TACC password at the password prompt\n",
      "2) At the TACC Token prompt, enter your 6-digit code followed by <return>.  \n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "          Welcome to the Stampede2 Supercomputer                 \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "No reservation for this job\n",
      "--> Verifying valid submit host (login4)...OK\n",
      "--> Verifying valid jobname...OK\n",
      "--> Enforcing max jobs per user...OK\n",
      "--> Verifying availability of your home dir (/home1/06548/tg858476)...OK\n",
      "--> Verifying availability of your work dir (/work/06548/tg858476/stampede2)...OK\n",
      "--> Verifying availability of your scratch dir (/scratch/06548/tg858476)...OK\n",
      "--> Verifying valid ssh keys...OK\n",
      "--> Verifying access to desired queue (normal)...OK\n",
      "--> Verifying job request is within current queue limits...OK\n",
      "--> Checking available allocation (TG-MCB090174)...OK\n",
      "Submitted batch job 4448178\n",
      "\n",
      "DEBUG:pilot-streaming:Found SLURM Job ID: 4448178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Job: 4448178 State : Running\n",
      "look for configs in: /home1/06548/tg858476/work/kafka-dbd88714-e6d8-11e9-8599-15f796122f42/config\n",
      "['broker-0']\n",
      "Kafka Config: /home1/06548/tg858476/work/kafka-dbd88714-e6d8-11e9-8599-15f796122f42/config (Fri Oct  4 13:58:30 2019)\n",
      "{'broker.id': '0', 'listeners': 'PLAINTEXT://c401-013:9092', 'zookeeper.connect': 'c401-013:2181', 'zookeeper.connection.timeout.ms': '6000'}\n",
      "CPU times: user 82.8 ms, sys: 200 ms, total: 282 ms\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kafka_pilot = pilot.streaming.PilotComputeService.create_pilot(pilot_compute_description)\n",
    "kafka_pilot.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look for configs in: /home1/06548/tg858476/work/kafka-dbd88714-e6d8-11e9-8599-15f796122f42/config\n",
      "['broker-0']\n",
      "Kafka Config: /home1/06548/tg858476/work/kafka-dbd88714-e6d8-11e9-8599-15f796122f42/config (Fri Oct  4 13:58:30 2019)\n",
      "{'broker.id': '0', 'listeners': 'PLAINTEXT://c401-013:9092', 'zookeeper.connect': 'c401-013:2181', 'zookeeper.connection.timeout.ms': '6000'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'master_url': 'c401-013:2181',\n",
       " 'details': {'broker.id': '0',\n",
       "  'listeners': 'PLAINTEXT://c401-013:9092',\n",
       "  'zookeeper.connect': 'c401-013:2181',\n",
       "  'zookeeper.connection.timeout.ms': '6000'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_pilot.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_pilot.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distributed\n",
    "\n",
    "pilot_compute_description = {\n",
    "    \"resource\":RESOURCE_URL,\n",
    "    \"working_directory\": WORKING_DIRECTORY,\n",
    "    \"number_of_nodes\": 1,\n",
    "    \"cores_per_node\": 48,\n",
    "    \"dask_cores\" : 24,\n",
    "    \"project\": \"TG-MCB090174\",\n",
    "    \"queue\": \"normal\",\n",
    "    \"walltime\": 359,\n",
    "    \"type\":\"dask\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pilot-streaming:Pilot-Streaming SLURM: Parsing job description: {'executable': 'python', 'arguments': ['-m', 'pilot.plugins.dask.bootstrap_dask', ' -p ', '24'], 'working_directory': '/home1/06548/tg858476/work/dask-718a0378-e6d9-11e9-8599-15f796122f42', 'output': 'dask_job_dask-718a0378-e6d9-11e9-8599-15f796122f42.stdout', 'error': 'dask_job_dask-718a0378-e6d9-11e9-8599-15f796122f42.stderr', 'number_of_nodes': 1, 'cores_per_node': 48, 'project': 'TG-MCB090174', 'reservation': None, 'queue': 'normal', 'walltime': 359, 'pilot_compute_description': {'resource': 'slurm+ssh://login4.stampede2.tacc.utexas.edu', 'working_directory': '/home1/06548/tg858476/work', 'number_of_nodes': 1, 'cores_per_node': 48, 'dask_cores': 24, 'project': 'TG-MCB090174', 'queue': 'normal', 'walltime': 359, 'type': 'dask'}}\n",
      "DEBUG:pilot-streaming:Submit pilot job to: slurm+ssh://login4.stampede2.tacc.utexas.edu\n",
      "DEBUG:pilot-streaming:Type Job IDps-718ac\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory: /home1/06548/tg858476/work\n",
      "dask-718a0378-e6d9-11e9-8599-15f796122f42/home1/06548/tg858476/work\n",
      "/tmp/tmpaypyn7_o\n",
      "Submission of Job Command: ssh login4.stampede2.tacc.utexas.edu sbatch  tmpaypyn7_o\n",
      "Cleanup: ssh login4.stampede2.tacc.utexas.edu rm tmpaypyn7_o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pilot-streaming:Pilot-Streaming SLURM: SSH run job finished\n",
      "DEBUG:pilot-streaming:Output - \n",
      "To access the system:\n",
      "\n",
      "1) If not using ssh-keys, please enter your TACC password at the password prompt\n",
      "2) At the TACC Token prompt, enter your 6-digit code followed by <return>.  \n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "          Welcome to the Stampede2 Supercomputer                 \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "No reservation for this job\n",
      "--> Verifying valid submit host (login4)...OK\n",
      "--> Verifying valid jobname...OK\n",
      "--> Enforcing max jobs per user...OK\n",
      "--> Verifying availability of your home dir (/home1/06548/tg858476)...OK\n",
      "--> Verifying availability of your work dir (/work/06548/tg858476/stampede2)...OK\n",
      "--> Verifying availability of your scratch dir (/scratch/06548/tg858476)...OK\n",
      "--> Verifying valid ssh keys...OK\n",
      "--> Verifying access to desired queue (normal)...OK\n",
      "--> Verifying job request is within current queue limits...OK\n",
      "--> Checking available allocation (TG-MCB090174)...OK\n",
      "Submitted batch job 4448193\n",
      "\n",
      "DEBUG:pilot-streaming:Found SLURM Job ID: 4448193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Job: 4448193 State : Running\n",
      "init distributed client\n",
      "Connect to Dask: tcp://c402-031:8786\n",
      "Dask Client Connect Attempt 0 failed\n",
      "init distributed client\n",
      "Connect to Dask: tcp://c402-031:8786\n",
      "Dask Client Connect Attempt 1 failed\n",
      "init distributed client\n",
      "Connect to Dask: tcp://c402-031:8786\n",
      "Dask Client Connect Attempt 2 failed\n",
      "init distributed client\n",
      "Connect to Dask: tcp://c402-031:8786\n",
      "{'type': 'Scheduler', 'id': 'Scheduler-b6c658a6-cae4-4da6-bed9-561e955275ea', 'address': 'tcp://206.76.194.69:8786', 'services': {'dashboard': 8787}, 'workers': {'tcp://206.76.194.69:39148': {'type': 'Worker', 'id': 'tcp://206.76.194.69:39148', 'host': '206.76.194.69', 'resources': {}, 'local_directory': '/home1/06548/tg858476/worker-bcamhi7t', 'name': 'tcp://206.76.194.69:39148', 'nthreads': 24, 'memory_limit': 92000000000, 'last_seen': 1570215811.4124334, 'services': {'dashboard': 42111}, 'metrics': {'cpu': 0.0, 'memory': 83361792, 'time': 1570215811.2157397, 'read_bytes': 0.0, 'write_bytes': 0.0, 'num_fds': 22, 'executing': 0, 'in_memory': 0, 'ready': 0, 'in_flight': 0, 'bandwidth': 100000000}, 'nanny': 'tcp://206.76.194.69:39441'}}}\n",
      "CPU times: user 15.1 s, sys: 1.66 s, total: 16.8 s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dask_pilot = pilot.streaming.PilotComputeService.create_pilot(pilot_compute_description)\n",
    "dask_pilot.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'master_url': 'tcp://c402-031:8786', 'web_ui_url': 'http://c402-031:8787'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_pilot.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Scheduler',\n",
       " 'id': 'Scheduler-b6c658a6-cae4-4da6-bed9-561e955275ea',\n",
       " 'address': 'tcp://206.76.194.69:8786',\n",
       " 'services': {'dashboard': 8787},\n",
       " 'workers': {'tcp://206.76.194.69:39148': {'type': 'Worker',\n",
       "   'id': 'tcp://206.76.194.69:39148',\n",
       "   'host': '206.76.194.69',\n",
       "   'resources': {},\n",
       "   'local_directory': '/home1/06548/tg858476/worker-bcamhi7t',\n",
       "   'name': 'tcp://206.76.194.69:39148',\n",
       "   'nthreads': 24,\n",
       "   'memory_limit': 92000000000,\n",
       "   'last_seen': 1570216310.4432776,\n",
       "   'services': {'dashboard': 42111},\n",
       "   'metrics': {'cpu': 10.0,\n",
       "    'memory': 107163648,\n",
       "    'time': 1570216309.9269888,\n",
       "    'read_bytes': 136285.44632290705,\n",
       "    'write_bytes': 135804.07783893382,\n",
       "    'num_fds': 25,\n",
       "    'executing': 0,\n",
       "    'in_memory': 0,\n",
       "    'ready': 0,\n",
       "    'in_flight': 0,\n",
       "    'bandwidth': 100000000},\n",
       "   'nanny': 'tcp://206.76.194.69:39441'}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import distributed\n",
    "dask_client  = distributed.Client(dask_pilot.get_details()['master_url'])\n",
    "dask_client.scheduler_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_client.gather(dask_client.map(lambda a: a*a, range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T18:00:04.950564Z",
     "start_time": "2017-12-28T17:59:22.095228Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### Required Spark configuration that needs to be provided before pyspark is imported and JVM started\n",
    "#os.environ[\"SPARK_LOCAL_IP\"]='129.114.58.101' #must be done before pyspark is loaded\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "pilot_compute_description = {\n",
    "   \"resource\":RESOURCE_URL,\n",
    "    \"working_directory\": WORKING_DIRECTORY,\n",
    "    \"number_of_nodes\": 1,\n",
    "    \"cores_per_node\": 48,\n",
    "    \"project\": \"TG-MCB090174\",\n",
    "    \"queue\": \"normal\",\n",
    "    \"walltime\": 359,\n",
    "    \"type\":\"spark\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Spark Cluster and Wait for Startup Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pilot-streaming:Pilot-Streaming SLURM: Parsing job description: {'executable': 'python', 'arguments': ['-m', 'pilot.plugins.spark.bootstrap_spark'], 'working_directory': '/home1/06548/tg858476/work/spark-e33511ce-e6da-11e9-8599-15f796122f42', 'output': 'spark_job_spark-e33511ce-e6da-11e9-8599-15f796122f42.stdout', 'error': 'spark_job_spark-e33511ce-e6da-11e9-8599-15f796122f42.stderr', 'number_of_nodes': 1, 'cores_per_node': 48, 'project': 'TG-MCB090174', 'reservation': None, 'queue': 'normal', 'walltime': 359}\n",
      "DEBUG:pilot-streaming:Submit pilot job to: slurm+ssh://login4.stampede2.tacc.utexas.edu\n",
      "DEBUG:pilot-streaming:Type Job IDps-e3359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory: /home1/06548/tg858476/work\n",
      "/tmp/tmpy1kpepq3\n",
      "Submission of Job Command: ssh login4.stampede2.tacc.utexas.edu sbatch  tmpy1kpepq3\n",
      "Cleanup: ssh login4.stampede2.tacc.utexas.edu rm tmpy1kpepq3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pilot-streaming:Pilot-Streaming SLURM: SSH run job finished\n",
      "DEBUG:pilot-streaming:Output - \n",
      "To access the system:\n",
      "\n",
      "1) If not using ssh-keys, please enter your TACC password at the password prompt\n",
      "2) At the TACC Token prompt, enter your 6-digit code followed by <return>.  \n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "          Welcome to the Stampede2 Supercomputer                 \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "No reservation for this job\n",
      "--> Verifying valid submit host (login4)...OK\n",
      "--> Verifying valid jobname...OK\n",
      "--> Enforcing max jobs per user...OK\n",
      "--> Verifying availability of your home dir (/home1/06548/tg858476)...OK\n",
      "--> Verifying availability of your work dir (/work/06548/tg858476/stampede2)...OK\n",
      "--> Verifying availability of your scratch dir (/scratch/06548/tg858476)...OK\n",
      "--> Verifying valid ssh keys...OK\n",
      "--> Verifying access to desired queue (normal)...OK\n",
      "--> Verifying job request is within current queue limits...OK\n",
      "--> Checking available allocation (TG-MCB090174)...OK\n",
      "Submitted batch job 4448239\n",
      "\n",
      "DEBUG:pilot-streaming:Found SLURM Job ID: 4448239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Job: 4448239 State : Running\n",
      "Create Spark Context for URL: spark://206.76.194.222:7077\n",
      "Create Spark Context for URL: spark://206.76.194.222:7077\n",
      "CPU times: user 320 ms, sys: 208 ms, total: 528 ms\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark_pilot = pilot.streaming.PilotComputeService.create_pilot(pilot_compute_description)\n",
    "spark_pilot.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Spark Context for URL: spark://206.76.194.222:7077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spark_home': '/home1/06548/tg858476/work/spark-e33511ce-e6da-11e9-8599-15f796122f42/spark-2.4.4-bin-hadoop2.7',\n",
       " 'master_url': 'spark://206.76.194.222:7077',\n",
       " 'web_ui_url': 'http://206.76.194.222:8080'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_pilot.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf=pyspark.SparkConf()\n",
    "#conf.set(\"spark.driver.bindAddress\", \"129.114.58.101\")\n",
    "#sc = pyspark.SparkContext(master=\"spark://129.114.58.102:7077\", appName=\"dfas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T17:51:55.430862Z",
     "start_time": "2017-12-28T17:51:55.093479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Spark Context for URL: spark://206.76.194.222:7077\n"
     ]
    }
   ],
   "source": [
    "#os.environ[\"SPARK_LOCAL_IP\"]=\"129.114.58.101\"\n",
    "sc = spark_pilot.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd.map(lambda a: a*a).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_pilot.cancel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
