{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Pilot-Streaming on Wrangler\n",
    "\n",
    "In the first step we need to import all required packages and modules into the Python Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T17:59:21.386383Z",
     "start_time": "2017-12-28T17:59:21.364643Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pilot.streaming' from '/home/01131/tg804093/anaconda3/lib/python3.6/site-packages/pilot/streaming.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# System Libraries\n",
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "\n",
    "## logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    " \n",
    "\n",
    "# Pilot-Streaming\n",
    "import pilot.streaming\n",
    "sys.modules['pilot.streaming']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pilot-Compute Description is a simple key/value style description of the cluster environment that should be started. Alternatively, the commandline tool delivered with this package can be used:\n",
    "\n",
    "     pilot-streaming --resource=slurm://localhost --queue=normal --walltime=59 --number_cores=48 --framework spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T18:00:04.950564Z",
     "start_time": "2017-12-28T17:59:22.095228Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### Required Spark configuration that needs to be provided before pyspark is imported and JVM started\n",
    "#os.environ[\"SPARK_LOCAL_IP\"]='129.114.58.2' #must be done before pyspark is loaded\n",
    "import pyspark\n",
    "import os\n",
    "\n",
    "pilot_compute_description = {\n",
    "    \"resource\":\"slurm+ssh://login1.wrangler.tacc.utexas.edu\",\n",
    "    \"working_directory\": os.path.join('/work/01131/tg804093/wrangler/', \"work\"),\n",
    "    \"number_of_nodes\": 1,\n",
    "    \"cores_per_node\": 48,\n",
    "    \"project\": \"TG-MCB090174\",\n",
    "    \"queue\": \"normal\",\n",
    "    \"walltime\": 59,\n",
    "    \"type\":\"spark\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Spark Cluster and Wait for Startup Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pilot-streaming:Pilot-Streaming SLURM: Parsing job description\n",
      "DEBUG:pilot-streaming:Submit pilot job to: slurm+ssh://login1.wrangler.tacc.utexas.edu\n",
      "DEBUG:pilot-streaming:Type Job IDps-25490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp3c5oggdo\n",
      "Submission of Job Command: ssh login1.wrangler.tacc.utexas.edu sbatch  tmp3c5oggdo\n",
      "Cleanup: ssh login1.wrangler.tacc.utexas.edu rm tmp3c5oggdo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pilot-streaming:Pilot-Streaming SLURM: SSH run job finished\n",
      "DEBUG:pilot-streaming:Output - \n",
      "\n",
      "To access the system:\n",
      "\n",
      "1) If not using ssh-keys, please enter your TACC password at the password prompt\n",
      "2) At the TACC Token prompt, enter your 6-digit code followed by <return>.\n",
      "\n",
      "---------------------------------------------------------------\n",
      "          Welcome to the Wrangler Supercomputer                 \n",
      "---------------------------------------------------------------\n",
      "\n",
      "No reservation for this job\n",
      "--> Verifying valid submit host (login1)...OK\n",
      "--> Verifying valid jobname...OK\n",
      "--> Enforcing max jobs per user...OK\n",
      "--> Verifying availability of your home dir (/home/01131/tg804093)...OK\n",
      "--> Verifying availability of your work dir (/work/01131/tg804093/wrangler)...OK\n",
      "--> Verifying valid ssh keys...OK\n",
      "--> Verifying access to desired queue (normal)...OK\n",
      "--> Verifying job request is within current queue limits...OK\n",
      "--> Checking available allocation (TG-MCB090174)...OK\n",
      "Submitted batch job 91691\n",
      "\n",
      "DEBUG:pilot-streaming:Found SLURM Job ID: 91691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Job: 91691 State : Running\n",
      "Create Spark Context for URL: spark://129.114.58.103:7077\n",
      "Create Spark Context for URL: spark://129.114.58.103:7077\n",
      "CPU times: user 13.8 ms, sys: 23.4 ms, total: 37.2 ms\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark_pilot = pilot.streaming.PilotComputeService.create_pilot(pilot_compute_description)\n",
    "spark_pilot.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Spark Context for URL: spark://129.114.58.103:7077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spark_home': '/work/01131/tg804093/wrangler/work/spark-2548d178-efe1-11e8-8522-549f3509766c/spark-2.4.0-bin-hadoop2.7',\n",
       " 'master_url': 'spark://129.114.58.103:7077',\n",
       " 'web_ui_url': 'http://129.114.58.103:8080'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_pilot.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = pyspark.SparkContext(master=\"spark://129.114.58.135:7077\", appName=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T17:51:55.430862Z",
     "start_time": "2017-12-28T17:51:55.093479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Spark Context for URL: spark://129.114.58.103:7077\n"
     ]
    }
   ],
   "source": [
    "sc = spark_pilot.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd.map(lambda a: a*a).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pilot-streaming:Cancel SLURM job\n"
     ]
    }
   ],
   "source": [
    "spark_pilot.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_compute_description = {\n",
    "    \"resource\":\"slurm://localhost\",\n",
    "    \"working_directory\": os.path.join('/work/01131/tg804093/wrangler/', \"work\"),\n",
    "    \"number_of_nodes\": 1,\n",
    "    \"cores_per_node\": 48,\n",
    "    \"project\": \"TG-MCB090174\",\n",
    "    \"queue\": \"normal\",\n",
    "    \"walltime\": 59,\n",
    "    \"type\":\"kafka\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pilot-streaming:Pilot-Streaming SLURM: Parsing job description\n",
      "DEBUG:pilot-streaming:Submit pilot job to: slurm://localhost\n",
      "DEBUG:pilot-streaming:Type Job IDps-c97bd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpt27cwoql\n",
      "Submission of Job Command: ssh login1.wrangler.tacc.utexas.edu sbatch  tmpt27cwoql\n",
      "Cleanup: ssh login1.wrangler.tacc.utexas.edu rm tmpt27cwoql\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pilot-streaming:Pilot-Streaming SLURM: SSH run job finished\n",
      "DEBUG:pilot-streaming:Output - \n",
      "\n",
      "To access the system:\n",
      "\n",
      "1) If not using ssh-keys, please enter your TACC password at the password prompt\n",
      "2) At the TACC Token prompt, enter your 6-digit code followed by <return>.\n",
      "\n",
      "---------------------------------------------------------------\n",
      "          Welcome to the Wrangler Supercomputer                 \n",
      "---------------------------------------------------------------\n",
      "\n",
      "No reservation for this job\n",
      "--> Verifying valid submit host (login1)...OK\n",
      "--> Verifying valid jobname...OK\n",
      "--> Enforcing max jobs per user...OK\n",
      "--> Verifying availability of your home dir (/home/01131/tg804093)...OK\n",
      "--> Verifying availability of your work dir (/work/01131/tg804093/wrangler)...OK\n",
      "--> Verifying valid ssh keys...OK\n",
      "--> Verifying access to desired queue (normal)...OK\n",
      "--> Verifying job request is within current queue limits...OK\n",
      "--> Checking available allocation (TG-MCB090174)...OK\n",
      "Submitted batch job 91694\n",
      "\n",
      "DEBUG:pilot-streaming:Found SLURM Job ID: 91694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Job: 91694 State : Running\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kafka_pilot = pilot.streaming.PilotComputeService.create_pilot(pilot_compute_description)\n",
    "kafka_pilot.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'details': {'broker.id': '0',\n",
       "  'listeners': 'PLAINTEXT://c251-132:9092',\n",
       "  'zookeeper.connect': 'c251-132:2181',\n",
       "  'zookeeper.connection.timeout.ms': '6000'},\n",
       " 'master_url': 'c251-132:2181'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_pilot.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_pilot.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distributed\n",
    "\n",
    "pilot_compute_description = {\n",
    "    \"resource\":\"slurm://localhost\",\n",
    "    \"working_directory\": os.path.join('/work/01131/tg804093/wrangler/', \"work\"),\n",
    "    \"number_of_nodes\": 1,\n",
    "    \"cores_per_node\": 48,\n",
    "    \"project\": \"TG-MCB090174\",\n",
    "    \"queue\": \"normal\",\n",
    "    \"walltime\": 59,\n",
    "    \"type\":\"dask\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pilot-streaming:Pilot-Streaming SLURM: Parsing job description\n",
      "DEBUG:pilot-streaming:Submit pilot job to: slurm://localhost\n",
      "DEBUG:pilot-streaming:Type Job IDps-3ed06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpkz74oax4\n",
      "Submission of Job Command: ssh login1.wrangler.tacc.utexas.edu sbatch  tmpkz74oax4\n",
      "Cleanup: ssh login1.wrangler.tacc.utexas.edu rm tmpkz74oax4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pilot-streaming:Pilot-Streaming SLURM: SSH run job finished\n",
      "DEBUG:pilot-streaming:Output - \n",
      "\n",
      "To access the system:\n",
      "\n",
      "1) If not using ssh-keys, please enter your TACC password at the password prompt\n",
      "2) At the TACC Token prompt, enter your 6-digit code followed by <return>.\n",
      "\n",
      "---------------------------------------------------------------\n",
      "          Welcome to the Wrangler Supercomputer                 \n",
      "---------------------------------------------------------------\n",
      "\n",
      "No reservation for this job\n",
      "--> Verifying valid submit host (login1)...OK\n",
      "--> Verifying valid jobname...OK\n",
      "--> Enforcing max jobs per user...OK\n",
      "--> Verifying availability of your home dir (/home/01131/tg804093)...OK\n",
      "--> Verifying availability of your work dir (/work/01131/tg804093/wrangler)...OK\n",
      "--> Verifying valid ssh keys...OK\n",
      "--> Verifying access to desired queue (normal)...OK\n",
      "--> Verifying job request is within current queue limits...OK\n",
      "--> Checking available allocation (TG-MCB090174)...OK\n",
      "Submitted batch job 91058\n",
      "\n",
      "DEBUG:pilot-streaming:Found SLURM Job ID: 91058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Job: 91058 State : Queue\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pilot/streaming.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pilot/plugins/dask/cluster.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Failed\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dask_pilot = pilot.streaming.PilotComputeService.create_pilot(pilot_compute_description)\n",
    "dask_pilot.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'master_url': 'tcp://c251-135:8786', 'web_ui_url': 'http://c251-135:8787'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_pilot.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': 'tcp://129.114.58.135:8786',\n",
       " 'id': 'Scheduler-363ae53b-1276-4ffc-bdc7-70b1aeb4283a',\n",
       " 'services': {'bokeh': 8787},\n",
       " 'type': 'Scheduler',\n",
       " 'workers': {'tcp://129.114.58.135:41796': {'cpu': 8.0,\n",
       "   'executing': 0,\n",
       "   'host': '129.114.58.135',\n",
       "   'in_flight': 0,\n",
       "   'in_memory': 10,\n",
       "   'last-seen': 1515036799.385555,\n",
       "   'last-task': 1515036750.2663264,\n",
       "   'local_directory': '/home/01131/tg804093/dask-worker-space/worker-lkiSY_',\n",
       "   'memory': 103673856,\n",
       "   'memory_limit': 134778585088,\n",
       "   'name': 'tcp://129.114.58.135:41796',\n",
       "   'ncores': 48,\n",
       "   'num_fds': 24,\n",
       "   'pid': 44991,\n",
       "   'read_bytes': 158293.4720896321,\n",
       "   'ready': 0,\n",
       "   'services': {'bokeh': 8789, 'nanny': 42225},\n",
       "   'time': 1515036798.885611,\n",
       "   'time-delay': 0.00038909912109375,\n",
       "   'write_bytes': 158293.4720896321}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import distributed\n",
    "dask_client  = distributed.Client(dask_pilot.get_details()['master_url'])\n",
    "dask_client.scheduler_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_client.gather(dask_client.map(lambda a: a*a, range(10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
